{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from amseg.amharicSegmenter import AmharicSegmenter\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read labeled data from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read labeled data from a text file\n",
    "with open('C:/Users/Administrator/Documents/10Academy/week 5/Technical Content/Data/labeled_telegram_product_price_location.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process lines as needed\n",
    "data = [line.strip().split('\\t') for line in lines]  # Adjust the split based on your delimiter\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Process lines: separate tokens and labels\n",
    "data = [line.strip().split() for line in lines if line.strip()]  # Split based on spaces\n",
    "tokens = [item[0] for item in data]  # Extract tokens\n",
    "labels = [item[1] for item in data]  # Extract labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c00951f3b421a83e66e3ce30a6234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\condaenv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d86968cf824608ac3c887ea55504dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b6cbd6143a43d6b97dc82e2a3b71a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc0dab1f06447c3bd29ffb705727e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\condaenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"xlm-roberta-base\"  # Change to any appropriate model from Hugging Face if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to tokenize and align labels for both Amharic and English texts\n",
    "def tokenize_and_align_labels(tokenizer, tokens, labels):\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for word, label in zip(tokens, labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)  # Tokenize the word\n",
    "        aligned_tokens.extend(tokenized_word)  # Add tokens to the list\n",
    "\n",
    "        # Assign the label to the first subtoken and 'O' to subsequent subtokens\n",
    "        aligned_labels.extend([label] + ['O'] * (len(tokenized_word) - 1))\n",
    "\n",
    "    return aligned_tokens, aligned_labels\n",
    "\n",
    "# Tokenize and align labels\n",
    "aligned_tokens, aligned_labels = tokenize_and_align_labels(tokenizer, tokens, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the first 20 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Tokens and Labels:\n",
      "▁3                   B-PRODUCT\n",
      "pc                   O\n",
      "s                    O\n",
      "▁si                  I-PRODUCT\n",
      "li                   O\n",
      "con                  O\n",
      "▁brush               I-PRODUCT\n",
      "▁spa                 I-PRODUCT\n",
      "tul                  O\n",
      "as                   O\n",
      "▁እስከ                 O\n",
      "▁2                   O\n",
      "60°                  O\n",
      "c                    O\n",
      "▁ሙ                   O\n",
      "ቀት                   O\n",
      "▁መቆ                  O\n",
      "ቆም                   O\n",
      "▁የሚችል                O\n",
      "▁ዋጋ                  I-PRICE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Aligned Tokens and Labels:\")\n",
    "for token, label in zip(aligned_tokens[:20], aligned_labels[:20]):\n",
    "    print(f\"{token:20} {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Amharic segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amharic Tokens and Labels:\n",
      "3pcs: B-PRODUCT\n",
      "silicon: I-PRODUCT\n",
      "brush: I-PRODUCT\n",
      "spatulas: I-PRODUCT\n",
      "እስከ: O\n",
      "260°c: O\n",
      "ሙቀት: O\n",
      "መቆቆም: O\n",
      "የሚችል: O\n",
      "ዋጋ-550ብር: I-PRICE\n",
      "አድራሻ: O\n",
      "ቁ.1: O\n",
      "ስሪ: O\n",
      "ኤም: O\n",
      "ሲቲ: O\n",
      "ሞል: O\n",
      "ሁለተኛ: O\n",
      "ፎቅ: O\n",
      "ቢሮ: O\n",
      "ቁ.: O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "segmenter = AmharicSegmenter(sent_punct, word_punct)\n",
    "\n",
    "# Function to align tokens with their respective labels for Amharic\n",
    "def align_tokens_with_labels(segmenter, tokens, labels):\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for word, label in zip(tokens, labels):\n",
    "        if any(char in word for char in 'አ-ፈ'):  # Check if the word contains Amharic characters\n",
    "            tokenized_word = segmenter.amharic_tokenizer(word)  # Tokenize the word using Amharic segmenter\n",
    "        else:\n",
    "            tokenized_word = [word]  # Keep the word as it is if not Amharic\n",
    "\n",
    "        aligned_tokens.extend(tokenized_word)  # Add tokens to the list\n",
    "        aligned_labels.extend([label] + ['O'] * (len(tokenized_word) - 1))\n",
    "\n",
    "    return aligned_tokens, aligned_labels\n",
    "\n",
    "# Align tokens and labels for Amharic\n",
    "new_tokens, new_labels = align_tokens_with_labels(segmenter, tokens, labels)\n",
    "\n",
    "# Output the first 20 aligned tokens and labels\n",
    "print(\"Amharic Tokens and Labels:\")\n",
    "for token, label in zip(new_tokens[:20], new_labels[:20]):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the final tokens and labels to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_df = pd.DataFrame({'Token': new_tokens, 'Label': new_labels})\n",
    "output_df.to_csv('C:/Users/Administrator/Documents/kifiya/Week_5/final_tokens_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results from index 50 to 80 with formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Results from Index 50 to 80:\n",
      "Slicer               I-PRODUCT\n",
      "ጊዜ                   O\n",
      "ቆጣቢ                  O\n",
      "ስላይስ                 O\n",
      "ማድረጊያ                O\n",
      "ለእጅ                  O\n",
      "ሴፍቲ                  O\n",
      "ተመራጭ                 O\n",
      "ለድንች                 O\n",
      "ለካሮትና                O\n",
      "ሌሎች                  O\n",
      "አታክልቶች               O\n",
      "ተመራጭ                 O\n",
      "ጥራት                  O\n",
      "ያለው                  O\n",
      "ዕቃ                   O\n",
      "ዋጋ፦                  I-PRICE\n",
      "1,200                O\n",
      "ብር                   I-PRICE\n",
      "አድራሻ                 O\n",
      "ቁ.1                  O\n",
      "ስሪ                   O\n",
      "ኤም                   O\n",
      "ሲቲ                   O\n",
      "ሞል                   O\n",
      "ሁለተኛ                 O\n",
      "ፎቅ                   O\n",
      "ቢሮ                   O\n",
      "ቁ.                   O\n",
      "SL-05A(ከ             O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Display Results from Index 50 to 80:\")\n",
    "for token, label in zip(new_tokens[50:80], new_labels[50:80]):\n",
    "    print(f\"{token:<20} {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset for additional processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bacb6195e54b47a1baffa4460cc056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\condaenv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\datasets--israel--Amharic-News-Text-classification-Dataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb50ac1b15924a7590bcaa91a5ef88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112106f2dfad4f6282b260384e4ba3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/37.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176fe5164f1f483fb406a3366e0eeebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/41186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa8a9d71e494384b092c85eb2a0dfee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"israel/Amharic-News-Text-classification-Dataset\")\n",
    "\n",
    "# Extract the text from the dataset\n",
    "texts = dataset['train']['article']\n",
    "\n",
    "# Filter out None values and prepare the data for SentencePiece\n",
    "filtered_texts = [text for text in texts if text is not None]  # Remove None values\n",
    "text_data = \"\\n\".join(filtered_texts)\n",
    "\n",
    "# Save to a temporary file\n",
    "with open('temp_text.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SentencePiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train('--input=temp_text.txt --model_prefix=AmharicSPM --vocab_size=100000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece Tokenization Results:\n",
      "['▁ቁ', '.', '2', '▁ለቡ', '▁መዳ', 'ህ', 'ኒ', 'ዓ', 'ለም', '▁and', '▁here', '▁is', '▁some', '▁Engl', 'ish', '▁', 'te', 'x', 't', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize SentencePiece tokenizer\n",
    "tokenizer_spm = spm.SentencePieceProcessor(model_file='AmharicSPM.model')\n",
    "\n",
    "# Tokenize the input text with SentencePiece\n",
    "text = \"ቁ.2 ለቡ መዳህኒዓለም and here is some English text.\"\n",
    "tokens_spm = tokenizer_spm.encode(text, out_type=str)  # Use encode method for tokenization\n",
    "\n",
    "# Print the tokenized words\n",
    "print(\"SentencePiece Tokenization Results:\")\n",
    "print(tokens_spm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process lines: separate tokens and labels again for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [line.strip().split() for line in lines if line.strip()]  # Split based on spaces\n",
    "tokens = [item[0] for item in data]  # Extract tokens\n",
    "labels = [item[1] for item in data]  # Extract labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and alignment with SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokenization Results (Index 50 to 80):\n",
      "ዛም                        10519      O\n",
      "⁇                         0          O\n",
      "ሞል                        618        O\n",
      "2ኛ                        29060      O\n",
      "ፎቅ                        12940      O\n",
      "ቢሮ                        531        O\n",
      "ቁጥር                       2468       O\n",
      "⁇                         0          O\n",
      "214                       6561       O\n",
      "ለቡ                        1110       I-LOC\n",
      "ቅርንጫፍ0973611819           0          O\n",
      "0909522840                49632      O\n",
      "0923350054                1109       O\n",
      "በTelegram                 4364       O\n",
      "ለማዘዝ                      8          O\n",
      "ይጠቀሙ                      0          O\n",
      "⁇                         2778       O\n",
      "shager                    5          O\n",
      "⁇                         0          O\n",
      "onlinestore               54941      O\n",
      "ለተጨማሪ                     190        O\n",
      "ማብራሪያ                     77         O\n",
      "የቴሌግራም                    6723       O\n",
      "ገፃችን                      226        O\n",
      "https                     108        O\n",
      "⁇                         0          O\n",
      "t                         68146      O\n",
      "⁇                         29060      O\n",
      "me                        2271       O\n",
      "⁇                         11865      O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_align_labels_spm(tokenizer_spm, tokens, labels):\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "    token_ids = []\n",
    "\n",
    "    for word, label in zip(tokens, labels):\n",
    "        tokenized_ids = tokenizer_spm.encode(word, out_type=int)  # Get token IDs\n",
    "        tokenized_words = tokenizer_spm.decode(tokenized_ids).split()  # Decode back to words\n",
    "\n",
    "        aligned_tokens.extend(tokenized_words)  # Add decoded words to the list\n",
    "        token_ids.extend(tokenized_ids)  # Add token IDs to the list\n",
    "\n",
    "        aligned_labels.extend([label] + ['O'] * (len(tokenized_words) - 1))\n",
    "\n",
    "    return aligned_tokens, token_ids, aligned_labels\n",
    "\n",
    "# Tokenize and align labels using SentencePiece\n",
    "custom_tokens, custom_ids, custom_labels = tokenize_and_align_labels_spm(tokenizer_spm, tokens, labels)\n",
    "\n",
    "# Display results from index 50 to 80 with tokenized words, their IDs, and labels\n",
    "print(\"Custom Tokenization Results (Index 50 to 80):\")\n",
    "for token, token_id, label in zip(custom_tokens[50:80], custom_ids[50:80], custom_labels[50:80]):\n",
    "    print(f\"{token:<25} {token_id:<10} {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
